{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesca/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data set containing customers reviews on musical instruments. In this notebook we will work on text data to create a classification of the users rating (source: http://jmcauley.ucsd.edu/data/amazon/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/Musical_Instruments_5.json',lines=True)\n",
    "# for the moment we will only take reviewText and overall\n",
    "df = data[['reviewText','overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "\n",
       "                                 summary  unixReviewTime  \n",
       "0                                   good      1393545600  \n",
       "1                                   Jake      1363392000  \n",
       "2                   It Does The Job Well      1377648000  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4  No more pops when I record my vocals.      1392940800  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Not much to write about here, but it does exac...        5\n",
       "1  The product does exactly as it should and is q...        5\n",
       "2  The primary job of this device is to block the...        5\n",
       "3  Nice windscreen protects my MXL mic and preven...        5\n",
       "4  This pop filter is great. It looks and perform...        5"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "- Calculate the percentage of data belonging to each rating class (from 1 to 5).\n",
    "- Which problems and difficulties could we face in the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a couple of examples of bad reviews (overall=1 or 2) and of good reviews (overall=5 or 4) to have a feeling how users write..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.reviewText.values\n",
    "y = df.overall.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr shape (9234,)\n",
      "X_te shape (1027,)\n",
      "y_tr shape (9234,)\n",
      "y_te shape (1027,)\n"
     ]
    }
   ],
   "source": [
    "print('X_tr shape',np.shape(X_tr))\n",
    "print('X_te shape',np.shape(X_te))\n",
    "print('y_tr shape',np.shape(y_tr))\n",
    "print('y_te shape',np.shape(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10261"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract numerical features from the text so that we can use them for classification.\n",
    "One way to achieve this is to use `sklearn.feature_extraction.text.CountVectorizer`: this scikit-learn class find tokens (words) in the text and create a matrix of occurrences counts. Another possibility is to use `sklearn.feature_extraction.text.TfidfVectorizer`: this method returns *weights* of words instead of the absolute count based on the appearence of each word in the all corpus of documents. The idea is that word that appears often in a corpus are not very significant to make predictions and should be given a lower weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "https://github.com/logicai-io/pipelines-sklearn\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - Would you make use of stop_words? Check ```CountVectorizer``` documentation\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "<div>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "------------------\n",
    "\n",
    "Stemming and Lemmatization are two linguistic normalization techniques that consists on grouping together words that derivate from the same origing and have the same meaning:\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "\n",
    "**Stemming** works by reducing a group of words to the same *stem* based on the origin of the words in the group. The stem does not necessarly have to be a word in the language. With **lemmatization**, on the other hand, makes sure that the root word is a proper word (*lemma*) in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesca/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/francesca/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/francesca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('universality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('universally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universality'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('universality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'university'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('university')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'universally'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer.lemmatize('universally',pos=wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "Extend the function `clean_text` created above to a function that, after cleaning the text, applies a stemming technique to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "<div>\n",
    "def clean_text_stem(text):\n",
    "    t = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(t)    # alternative: [w for w in t.split()]\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens] \n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "    \n",
    "vectorizers = [\n",
    "     (\"simple\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "    ,(\"preprocessing + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "    ,(\"stemming + min_df=10 + stop_words\",\n",
    "          CountVectorizer(preprocessor=clean_text_stem,\n",
    "                          min_df=10,\n",
    "                          stop_words=stop_words.ENGLISH_STOP_WORDS))\n",
    "]\n",
    "\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "    print('and' in vect.get_feature_names())\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and a classifier (ex `RandomForestClassifier`)\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier\n",
    "3. Play with text preprocessing in ```CountVectorizer``` to see if the model improves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "<div>\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.20      0.00      0.01       202\n",
      "          2       0.33      0.01      0.02       225\n",
      "          3       0.20      0.03      0.05       696\n",
      "          4       0.33      0.12      0.17      1862\n",
      "          5       0.70      0.95      0.80      6249\n",
      "\n",
      "avg / total       0.57      0.67      0.58      9234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    2.4s remaining:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', RandomForestClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check some mis-predicted reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=10,preprocessor=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train dataframe from X_tr (this can be avoided by using train_test_split above directly on the df)\n",
    "df_tr = pd.DataFrame(X_tr,columns=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tr_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_ = cv.fit_transform(X_tr)\n",
    "df_tr_ = pd.DataFrame(X_tr_.todense(), columns=cv.get_feature_names())\n",
    "df_tr_['overall'] = y_tr\n",
    "df_tr_['preds'] = preds\n",
    "df_tr_['reviewText'] = df_tr['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered two of these stands and they work very well! Definitely keeps my guitars a lot safer than having them leaning against things.'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ordered two of these stands and they work ve...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Works well, pretty straight forward. A little ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Installed on two guitars and I love them! I ca...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been using this for roughly a year now, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pedal essentialy balances out the high, m...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall  preds\n",
       "0  I ordered two of these stands and they work ve...        5      5\n",
       "1  Works well, pretty straight forward. A little ...        5      5\n",
       "2  Installed on two guitars and I love them! I ca...        5      5\n",
       "3  I've been using this for roughly a year now, a...        4      4\n",
       "4  This pedal essentialy balances out the high, m...        5      5"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_[['reviewText','overall','preds']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>I recently bought this for my first electric g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>This guitar strap was so inexpensive compared ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>I bought a case of these.  I can't handle #12 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>I have to admit this up front - I'm a terrible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>I bough this amp for my 9 year old son who is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviewText\n",
       "128   I recently bought this for my first electric g...\n",
       "1849  This guitar strap was so inexpensive compared ...\n",
       "2147  I bought a case of these.  I can't handle #12 ...\n",
       "2202  I have to admit this up front - I'm a terrible...\n",
       "3219  I bough this amp for my 9 year old son who is ..."
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check text that contains the word \"terrible\" but they are actually labelled as positive\"\n",
    "df_tr_[(df_tr_.terrible>0) & (df_tr_.overall==5)][['reviewText']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I recently bought this for my first electric guitar and I thoroughly enjoy it. It has a lot of power and noise behind it. It is a little starter amp, just in case the picture was a little deceiving. I cranked it up quite loud and it produces clean sound without terrible distortion. With distortion, it sounds great. It is a solid piece of work, and it makes a great practice amp and possibly a great starter amp.'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_[(df_tr_.terrible>0) & (df_tr_.overall==5)].iloc[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "<div>\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "             \n",
    "grid_clf = GridSearchCV(clf, params, scoring='f1_weighted', n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "#    \n",
    "#print(\"Randomized search\")\n",
    "#print()\n",
    "#    \n",
    "#params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "#          'vect__analyzer': [\"word\",\"char\"],\n",
    "#          'model__lr__dimensions': [100, 200]}\n",
    "#\n",
    "#grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "#grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "#\n",
    "#best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "#\n",
    "#for params, score, _ in best_params:\n",
    "#    print(score, params)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract further information from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text review contains a number of information that can be used for classification.\n",
    "Examples:\n",
    "- length of text\n",
    "- punctuation\n",
    "- emoji usage \n",
    "\n",
    "Example:\n",
    "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesca/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overall\n",
       "1    391.0\n",
       "2    363.0\n",
       "3    351.5\n",
       "4    319.0\n",
       "5    265.0\n",
       "Name: review_length, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review_length'] = df['reviewText'].apply(lambda x: len(x))\n",
    "df.groupby('overall')['review_length'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "---------\n",
    "\n",
    "1. Create a custom Transformer that extract the length of the review \n",
    "2. Create a custom Transformer that detect number of positive emoticons \n",
    "3. Add the transformers above to the features extraction part in the pipeline above. **ATTENTION**: Make sure the transform methods return a sparse matrix (use `from scipy import sparse` `sparse.csr_matrix`), otherwise you will get dimensions error from numpy concatenation method used in FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Click here to see the solution**\n",
    "<div>\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "\n",
    "class LengthExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        l = np.array([len(x) for x in X])\n",
    "        return sparse.csr_matrix(l).T\n",
    "    \n",
    "\n",
    "class CountEmojis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        l_pos = np.array([len(re.findall('(:\\)|:D|;\\)|:\\])', x)) for x in X])\n",
    "        return sparse.csr_matrix(l_pos).T\n",
    "        #return l_pos\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "        ('len', LengthExtractor())\n",
    "    ])),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
